[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/penguin_blog.html",
    "href": "posts/penguin_blog.html",
    "title": "Palmer Penguins Blog Post",
    "section": "",
    "text": "Palmer Penguins Blog Post\nAbstract In this blog posts, I delve into some of the factors that we can use to predict species in the palmer penguins dataset. I used SelectKBest and going through all combinations to select my columns that I’d use to try to predict species. Although both SelectKBest and combinations method got similar levels of accuracy, they selected slightly different columns. Finally, I visualize my model to showcase how future data predictions would look.\nImporting Palmer Penguins Data Set\n\nimport pandas as pd\nimport numpy as np\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nHere I am prepareing the data by dropping columns that don’t makes sense to train on or are constant for the data set. I also convert columns like Island into boolean columns using pandas getDummies.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = df.drop([\"Stage\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nHere we can see what the new data looks like\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nTable It’s always important to see the sample size of the different columns that we’re testing for. In this dataset, for example, we see that we have significantly more Adelie and Gentoo Penguins than Chinstrap penguins. In fact, we have twice as many Adelie penguins as Chinstrap ones. This isn’t ideal to train on because our model may choose to priorities features that classify Adelie penguins. For example, a naive classifier that classified only Adelie penguins correctly would have 43% accuracy while one that only classified Chinstrap ones would have 21% accuracy. If these were the only two options then the model would pick the 43% accuracy even tho it has a massive tilt.\n\ntrain.groupby(\"Species\").aggregate(\"count\")\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n120\n120\n120\n120\n120\n120\n120\n120\n119\n119\n119\n119\n114\n110\n110\n23\n\n\nChinstrap penguin (Pygoscelis antarctica)\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n57\n56\n57\n0\n\n\nGentoo penguin (Pygoscelis papua)\n98\n98\n98\n98\n98\n98\n98\n98\n97\n97\n97\n97\n94\n96\n96\n0\n\n\n\n\n\n\n\nPlots This plot looks at the qualitative column “Island” and visualizes it to see if there would possibly be any trends that could be helpful. From the plot, we see that Chinstrap penguins are exclusively found on Dream, while Gentoo penguins are exclusively found on Biscoe Island.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, figsize = (8, 3.5))\n\nplot = sns.countplot(train, x = \"Island\", hue = \"Species\")\n\n\n\n\n\n\n\n\nMy second graph visualizes Culment Length vs Flipper Length to see if there’s a correlation between the two and species. We see that Adelie penguins tend to have small culmen lengths and flippper lengths. Gentoo penguins, on the other hand, tend to have medium to large culmen lengths and long flipper lengths. Lastly, Chinstrap penguins tend to have long culmen lengths and medium to small flipper lengths.\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", ax = ax[1])\n\n\n\n\n\n\n\n\nSelecting Features To select my features I try two different methods: SelectKBest and trying all possible combinations. Below we see my Select K Best implementation. I had to seperate out quantative and qualitative features so that SelectKBest didn’t choose 3 quantative features.\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif, chi2\n\ndef select_K_best(X, y, score_func, k):\n    selector = SelectKBest(score_func, k=k)\n    selector.fit(X, y)\n    return selector.get_feature_names_out()\n\n\nX_quant_selected = select_K_best(X_train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]], y_train, f_classif, 2)\n\nX_qual_selected = select_K_best(X_train[[\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]], y_train, chi2, 1)\n\n\nX_quant_selected\n\narray(['Culmen Length (mm)', 'Flipper Length (mm)'], dtype=object)\n\n\n\nX_qual_selected = [col for col in X_train.columns if X_qual_selected[0][0:4] in col]\nX_qual_selected\n\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nHere we see all the rows selected in through SelectKBest\n\nselectK_cols = X_quant_selected.tolist() + X_qual_selected\nselectK_cols\n\n['Culmen Length (mm)',\n 'Flipper Length (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']\n\n\nI also implemented running through all combinations and calculating accuracy based on a random forrest algorithm. This method is significantly more time consuming than select k best but should return the most optimal columns.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = RandomForestClassifier(n_estimators=10, random_state=20)\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Island\", \"Clutch Completion\", \"Sex\"]\nall_quant_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nmean_score = 0\ncomb_cols = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    clf.fit(X_train[cols], y_train)\n    score = cross_val_score(clf, X_train[cols], y_train, cv=5)\n    if score.mean() &gt; mean_score:\n      mean_score = score.mean()\n      comb_cols = cols\ncomb_cols = comb_cols[3:5] + comb_cols[0:3]\ncomb_cols\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']\n\n\nWhich Features to use To determine which algorithm to use, I test both possibility using cross val scores and using the random forrest classifier.\n\nselect_clf = RandomForestClassifier(n_estimators=40, random_state=40, max_depth=5)\nselect_clf.fit(X_train[selectK_cols], y_train)\nselect_score_clf = cross_val_score(select_clf, X_train[selectK_cols], y_train, cv=5)\nselect_score_clf.mean()\n\n0.9804675716440423\n\n\n\ncomb_clf = RandomForestClassifier(n_estimators=40, random_state=40, max_depth=5)\n\n\ncomb_clf.fit(X_train[comb_cols], y_train)\ncomb_score_clf = cross_val_score(comb_clf, X_train[comb_cols], y_train, cv=5)\ncomb_score_clf.mean()\n\n0.9804675716440423\n\n\nTesting My Data While the scores were similar, I decided to use the comb_clf. Now I’ll try to test it on the test data set.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\ncomb_clf.score(X_test[comb_cols], y_test)\n\n1.0\n\n\nYay! As we can see we achieved a score of 100%.\nVisualizing Results This next block of code, given to us from Phil, helps show how our model classifies data. The one interesting thing to note is that Island Dream graph has several sharp blue ‘inlets’ that helps it categories certain Gentoo data points. This isn’t ideal and shows possibilites of overfitting.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(comb_clf, X_train[comb_cols], y_train)\n\n\n\n\n\n\n\n\n\nplot_regions(comb_clf, X_test[comb_cols], y_test)\n\n\n\n\n\n\n\n\nSince the result has 100% accuracy, the confusion matrix won’t show much for the test set but it does show us that there aren’t an even amount of penguins types in the test set. This means that my algorithm could perform worse on under-represented test species.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = comb_clf.predict(X_test[comb_cols])\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nDiscussion I managed to achieve 100% accuracy for the test data! I found it interesting that the combination method and selectKBest selected different features, however there are a couple reasons I could think of why. First one is that selectKBest doesn’t take into account how features might interact with eachother. For example, two features may predict on part of the data really well while another feature may predict a different part slightly less well. The best algorithm would use both to train but selectKBest seems like it would only choose the two that help predict the same part of the data because they overall correlate better. I wonder how you can eliminate this weakness on datasets where you can’t go through every combination. Even with the combination features, from the visualizations we see that the model created isn’t perfect. The graph shows a couple slim lines that perfectly allow some data points to get correctly labeled which shows some weakness in the model. With a larger test dataset, I’m sure this overfitting wouldn’t hold for all data points in that area. In the future, I would love to try different models to see if and how they might come up with varying degrees of accuracy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Women in Data Science Blog Post\n\n\n\n\n\nBlog Post for CS0451\n\n\n\n\n\nMar 4, 2024\n\n\nOtis Milliken\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study\n\n\n\n\n\nBlog Post for CS0451\n\n\n\n\n\nMar 3, 2024\n\n\nOtis Milliken\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Blog Post\n\n\n\n\n\nBlog Post for the First Assignment in CS0451\n\n\n\n\n\nFeb 20, 2024\n\n\nOtis Milliken\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Women in Data Science.html",
    "href": "posts/Women in Data Science.html",
    "title": "Women in Data Science Blog Post",
    "section": "",
    "text": "Women’s in Data Science Blog Post\n\n\nAbstract\n\n\nReading Questions\nWhy is it a problem that women are underrepresented in computing, math, and engineering? For whom is it a problem?\nIt seems that the underrepresentation of women in these fields also hurt men. One finding from India found that male leaders held less implicit bias when they had female leadership alongside them (Beaman et al., 2009).\nHow is the representation and status of women in computing today different from the 1950s and 1960s? What are some of the forces that brought on this change?\nDuring World War 2, we actually saw a majority of women in the computing industry. This declined drastically after the war but in 1950s and 1960s the number of women in computing was similar to what it is today (around 26%). The percentaged reached a peak in the 1990s with around 35% of the computing workforce being women. Computing during the 1950s and 1960s was largely seen as an administrative extension and thereby women were encouraged to participate in large numbers. The decline after the 1990s is often attributed to the rise of personal computers which were seen as new toys for boys and the growing interest in the area. This early experience gave them an advantage as the computing market expanded.\nWhich of the barriers and unequal challenges described in the section “Why So Few?” can be eroded by events that spotlight the achievement of women in STEM?\nHaving spotlight on women in the industry is an important step to reducing stigma and feelings of isolation while also giving role models for women in computer science. A third of women in private-sector technical jobs feel extremeley isolated at work and four out of ten female engineers reported a lack of role models. The book mentions how seeing other women in leadership roles can actually help women from the harmful effect of stereotyping effect (Van Loo & Rydell, 2014). These effects have a cascade effect as increase in female representation leads to more representation which further reduces feelings of isolation and increases representation.\nThese talks also begin the process of building social networks which has predominantly been dominated by men. The book highlights how research has identified that these male-dominated social networks exclude women (Faulkner, 2009a). Having women talk about their professional experiences and connect with students begins the process of opening up the network to women.\n\n\nFirst Lightning Talk\nAmy Yuen’s talk discussed whether the United Nation Security Council is truly a Democratic Institution. On the onset, the voting rules seems to inherently favor the powerful since certain members are given veto powers. Why do other countries then spend significant resources and money on running for seats on the Security Council even if they have significantly less power? The talk analysis two different ways to look at democratic institutions: Institutional Rules which contains the voting inside the council and serving on the council; and Representation which looks at which issues are discussed and what the participation level of the non-permanent members are. The overall findings were that while the security council institutional rules aren’t very democratic, it is fairly representative and inclusive. From the talk, I learned about how one can use data for issues that are not necessarily data oriented. I wouldn’t at first think to use data analysis to come to a conclusion about the level of democracy of an institution since it feels more vague than other issues but now I do think it can be very useful to back up ones claims.\n\n\nSecond Lightning Talk Jessica L’Roe\nJessica L’Roe’s talk was more a cursery overview of some of the projects that she has worked on.\nIn College, only had one woman professor. Grew up in North Carolina and had to constantly fight expectations\nWork She’s Done:\nLand Registration in Brazil Registering people’s land to help prove culpability for deforestation\nFormalizing Gold Mining in Peru\nLandscape Changes around Kibale National Park, Uganda Who is planting tree and why are they planting trees. Lots owned by non local people Lots analysis in Uganda. Mother’s investing in education instead of land.\nWomen are not a rarity in the field anymore. Should go for it if you can.\n\n\nThird Lightning Talk\n\n\nKeynote\nCS + Stats + Domain Expertise Data Science is the Intersection of all three\nMany of different domain expertise\nSocial Studies taught me: Skill: Use Context to understand Primary Sources which will turn into Use Context to understand Data\nInstead of using only regression loss score. Also use a classification score - Showed many fewer patients in the control group score higher than the PTSD group - Context was necessary to come up with this technique\nDiscipline isn’t purely a topic but also a discourse. “Disciplines are communities”.\nComputer Scientist often used computer science research methods on humans. However these often are flawed. Need people from a variety of fields. - One example, frame questions with similar levels of abstraction.\nFairness Check before Data Scientist even fit the model. - How to make objective fairness check?\nData Collection and Model doesn’t have to be biased However the World is inherently biased which means we have to think about larger bias.\nDifferent answers to whether we can make fair algorithms - If you accept to lose accuracy, you can improve fairness - Some people just think it isn’t possible and should be tried - Still progress to made\nChoice of target variable (proxy for what we trying to predict) matters a lot\nLaws don’t move as fast as code because they need to be socially discussed. Code move super fast.\n\n\nNotes"
  },
  {
    "objectID": "posts/Replication Study.html",
    "href": "posts/Replication Study.html",
    "title": "Replication Study",
    "section": "",
    "text": "In this blog post, I followed along with the study by Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan (2019) called “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Essentially this blog posts steps through their methodology to understand how they got to the conclusions that they did. I found that there existed a black and white patient divide in the correlation between the mean number of chronic illness and risk percentile. I also found that there existed a disportionate cost of black patients compared to white patients.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\ndf[\"risk_percentile\"] = (df[\"risk_score_t\"].rank(pct=True) * 100).round()\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nchart = df.groupby([\"risk_percentile\", \"race\"]).aggregate({\"gagne_sum_t\" : \"mean\"}).reset_index()\n\nsns.scatterplot(data=chart, x=\"gagne_sum_t\", y=\"risk_percentile\", hue=\"race\")\nplt.xlabel(\"Mean Number of Chronic Illnesses\")\nplt.ylabel(\"Risk Percentile\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\n\nThe chart shows the White patients on average have to have less chronic illnesses to have higher risk. This means that white patients are more likely to be referred to treatment (high risk treatment) than black patients. For example, a white patient with a risk percentile of 60 needs on average around 1.1 chronic illnesses while a black patient would need to have around 1.8-1.9 chronic illness to be in that same risk percentile.\n\nchart2 = df.groupby([\"risk_percentile\", \"race\"]).aggregate({\"cost_t\" : \"mean\"}).reset_index()\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\nsns.scatterplot(data=chart2, x=\"risk_percentile\", y=\"cost_t\", hue=\"race\", ax=ax[0])\nax[0].set(xlabel =\"Percentile Risk\", ylabel = \"Medical Cost\", yscale=\"log\")\n\nchart3 = df.groupby([\"race\", \"gagne_sum_t\"]).aggregate({\"cost_t\" : \"mean\"}).reset_index()\n\nsns.scatterplot(data=chart3, x=\"gagne_sum_t\", y=\"cost_t\", hue=\"race\", ax=ax[1])\nax[1].set(xlabel =\"Chronic Of Illnesses\", ylabel = \"Medical Cost\", yscale=\"log\")\n\n\n\n\n\n\n\n\n\n\nThe paper argues that there is little difference in cost based on percentile risk shown in Figure 1 but that black patients tend to have lower costs for the number of chronic illnesses they have. I’m not sure the tables I created totally supports that hypothesis as both seem to have small differences between them. For example, between 7 and 14 chronic illnesses, it looks like black patients actually cost more than white patients. This could be because there’s not very many patients with over 5 chronic illnesses.\n\nfiveOrLess = 1 * (df[\"gagne_sum_t\"] &lt; 5).sum()\n\npercentage = fiveOrLess/len(df) * 100\nprint(\"Percentage of patients with 5 or less chronic illnesses: \", percentage.round(2), \"%\")\n\nPercentage of patients with 5 or less chronic illnesses:  93.0 %\n\n\n\ndf = df[df[\"cost_t\"] &gt; 0]\ndf[\"log_cost\"] = df[\"cost_t\"].transform(\"log\")\n\n\ndf[\"race_dummy\"] = 1 * (df[\"race\"] == \"black\")\ndf[\"race_dummy\"].mean()\n\n0.11329366348881353\n\n\n\nX_train = df[[\"race_dummy\", \"gagne_sum_t\"]]\ny_train = df[\"log_cost\"]\n\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nIn the next step, I loop through polynomial degrees up to 8\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nLR = LinearRegression()\n\nmax = 0\nfor degree in range(1, 9):\n    X_train = add_polynomial_features(X_train, degree)\n    cv_score_LR = cross_val_score(LR, X_train, y_train, cv=5)\n    print(f\"Degree {degree}: {cv_score_LR.mean()}\")\n\nDegree 1: 0.14538846793594346\nDegree 2: 0.14537952700038775\nDegree 3: 0.14699336604386143\nDegree 4: 0.146909632661908\nDegree 5: 0.1473340656385787\nDegree 6: 0.14776094174784324\nDegree 7: 0.1480739986116853\nDegree 8: 0.14811660549076067\n\n\n\nX_train = add_polynomial_features(X_train, 8)\nLR.fit(X_train, y_train)\nLR.score(X_train, y_train)\n\n0.14859554176104295\n\n\n\nprint(X_train.columns)\nprint(LR.coef_)\n\nIndex(['race_dummy', 'gagne_sum_t', 'poly_1', 'poly_2', 'poly_3', 'poly_4',\n       'poly_5', 'poly_6', 'poly_7'],\n      dtype='object')\n[-2.66862126e-01  3.36049611e-01  3.36049612e-01 -3.67776150e-01\n  1.30350450e-01 -2.24509569e-02  1.99855281e-03 -8.86256261e-05\n  1.54525291e-06]\n\n\n\nimport numpy as np\nnp.exp(LR.coef_[0])\n\n0.7657786454750596\n\n\nHere we see that the percentage cost of black patients is unsually high at around 76.6%.\n\n\n\nAs mentioned in the abstract, this paper found that there existed a black and white patient divide in the correlation between the mean number of chronic illness and risk percentile. It found that there existed a disportionate cost of black patients compared to white patients with 76.6%. I learned that to account for nonlinear relationships, we can add polynomials and then use a linear model on one of the order of that polynomials."
  },
  {
    "objectID": "posts/Replication Study.html#explaining-chart",
    "href": "posts/Replication Study.html#explaining-chart",
    "title": "Replication Study",
    "section": "",
    "text": "The chart shows the White patients on average have to have less chronic illnesses to have higher risk. This means that white patients are more likely to be referred to treatment (high risk treatment) than black patients. For example, a white patient with a risk percentile of 60 needs on average around 1.1 chronic illnesses while a black patient would need to have around 1.8-1.9 chronic illness to be in that same risk percentile.\n\nchart2 = df.groupby([\"risk_percentile\", \"race\"]).aggregate({\"cost_t\" : \"mean\"}).reset_index()\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\nsns.scatterplot(data=chart2, x=\"risk_percentile\", y=\"cost_t\", hue=\"race\", ax=ax[0])\nax[0].set(xlabel =\"Percentile Risk\", ylabel = \"Medical Cost\", yscale=\"log\")\n\nchart3 = df.groupby([\"race\", \"gagne_sum_t\"]).aggregate({\"cost_t\" : \"mean\"}).reset_index()\n\nsns.scatterplot(data=chart3, x=\"gagne_sum_t\", y=\"cost_t\", hue=\"race\", ax=ax[1])\nax[1].set(xlabel =\"Chronic Of Illnesses\", ylabel = \"Medical Cost\", yscale=\"log\")\n\n\n\n\n\n\n\n\n\n\nThe paper argues that there is little difference in cost based on percentile risk shown in Figure 1 but that black patients tend to have lower costs for the number of chronic illnesses they have. I’m not sure the tables I created totally supports that hypothesis as both seem to have small differences between them. For example, between 7 and 14 chronic illnesses, it looks like black patients actually cost more than white patients. This could be because there’s not very many patients with over 5 chronic illnesses.\n\nfiveOrLess = 1 * (df[\"gagne_sum_t\"] &lt; 5).sum()\n\npercentage = fiveOrLess/len(df) * 100\nprint(\"Percentage of patients with 5 or less chronic illnesses: \", percentage.round(2), \"%\")\n\nPercentage of patients with 5 or less chronic illnesses:  93.0 %\n\n\n\ndf = df[df[\"cost_t\"] &gt; 0]\ndf[\"log_cost\"] = df[\"cost_t\"].transform(\"log\")\n\n\ndf[\"race_dummy\"] = 1 * (df[\"race\"] == \"black\")\ndf[\"race_dummy\"].mean()\n\n0.11329366348881353\n\n\n\nX_train = df[[\"race_dummy\", \"gagne_sum_t\"]]\ny_train = df[\"log_cost\"]\n\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nIn the next step, I loop through polynomial degrees up to 8\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nLR = LinearRegression()\n\nmax = 0\nfor degree in range(1, 9):\n    X_train = add_polynomial_features(X_train, degree)\n    cv_score_LR = cross_val_score(LR, X_train, y_train, cv=5)\n    print(f\"Degree {degree}: {cv_score_LR.mean()}\")\n\nDegree 1: 0.14538846793594346\nDegree 2: 0.14537952700038775\nDegree 3: 0.14699336604386143\nDegree 4: 0.146909632661908\nDegree 5: 0.1473340656385787\nDegree 6: 0.14776094174784324\nDegree 7: 0.1480739986116853\nDegree 8: 0.14811660549076067\n\n\n\nX_train = add_polynomial_features(X_train, 8)\nLR.fit(X_train, y_train)\nLR.score(X_train, y_train)\n\n0.14859554176104295\n\n\n\nprint(X_train.columns)\nprint(LR.coef_)\n\nIndex(['race_dummy', 'gagne_sum_t', 'poly_1', 'poly_2', 'poly_3', 'poly_4',\n       'poly_5', 'poly_6', 'poly_7'],\n      dtype='object')\n[-2.66862126e-01  3.36049611e-01  3.36049612e-01 -3.67776150e-01\n  1.30350450e-01 -2.24509569e-02  1.99855281e-03 -8.86256261e-05\n  1.54525291e-06]\n\n\n\nimport numpy as np\nnp.exp(LR.coef_[0])\n\n0.7657786454750596\n\n\nHere we see that the percentage cost of black patients is unsually high at around 76.6%.\n\n\n\nAs mentioned in the abstract, this paper found that there existed a black and white patient divide in the correlation between the mean number of chronic illness and risk percentile. It found that there existed a disportionate cost of black patients compared to white patients with 76.6%. I learned that to account for nonlinear relationships, we can add polynomials and then use a linear model on one of the order of that polynomials."
  }
]